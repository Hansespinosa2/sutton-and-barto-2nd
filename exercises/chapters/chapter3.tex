\section{Finite Markov Decision Processes}

\subsection{Exercise 3.1}
\begin{itemize}
  \item An LLM is being fine tuned through reinforcement learning through human feedback. The states are a numerical representation of the inputted responses from a human and a numerical representation of the LLM's responses and it's weights. The action of the MDP is a shift in each of the weights and a novel generated prompt attempting to correct the response from earlier. The agent is given a reward that corresponds to a sentiment analysis of the human's response to the correction.
  \item A robot must learn to replicate the movements of a human. The states would be a representation of the human's movements. One way of this could be fixing dots on joints of the human and feeding the position of the dots to the robot. The robot then uses these dots and its own dots to move it's body in anticipation of the human's movements. The actions of the robot are movements of its possible joints, these may not correspond 100\% to the human dots or it's own dots if there are differences in how it can move it and how its movement is represented. The robot could be given a reward that is inversely proportional to the distance of its dots and the humans.  
  \item An AI must solve through a complex video game like final fantasy 7. The states would be incredibly complex as it would have to be designed with great detail and consideration. The state could include features such as: The screen (or a CNN output of the screen to reduce number of states), the items and materia in possession, the state of battle (0 if not in battle or 1 if in battle), the number of characters in the party. The actions could be the buttons that the player can press on a controller like start, o, x, and stick direction. The reward structure has a lot of freedom. The AI could be intrinsically motivated with curiosity through rewarding it based on an unseen state, or it could be given a bonus proportional to the EXP and gold earned.   
\end{itemize}

\subsection{Exercise 3.2}
It is not, the markov property assumes that the current state is a unique sufficient representation for the agent's past and its future. This could be violated in many scenarios. One of them could be a robot that is designed to escape a maze and it is given the state representation of the $[s_1,s_2,s_3,s_4]$ where $s_i = 1$ if the robot senses a wall within 3 feet of it's current position in each direction north, east, south, west. There are likely many parts of the maze that have identical state representations and yet are completely different parts of the maze. However, many MDPs can be turned into MDP if given a state that reflects it's history. For example, in this robot you could include a vector containing each of it's prior actions as a state and would therefore be able to know if it is repeating movements.

\subsection{Exercise 3.3}
The answer really depends on the underlying goal of the person creating the agent-environment system. In this driving scenario, it is likely that the person already knows where they are driving to, the just don't know how to get there. So an incredibly high level would not be a great place to draw the line because it wouldn't align the agent's actions with the designer's goals. 

\subsection{Exercise 3.4}

\begin{tabular}{ |c| c| c| c| c|}
  \hline
$s$ & $a$ & $s^\prime$ & $r$ & $p(s^\prime,r|s,a)$ \\
\hline
high & search & high & $r_{search}$ & $\alpha$ \\
high & search & low & $r_{search}$ & $1-\alpha$ \\
high & wait & high & $r_{wait}$ & $1$ \\
low & search & low & $r_{search}$ & $\beta$ \\
low & search & high & $-3$ & $1-\beta$ \\
low & recharge & high & $0$ & $1$ \\
low & wait & low & $r_{wait}$ & $1$ \\
\hline
\end{tabular}

