\section{Finite Markov Decision Processes}

\subsection{Exercise 3.1}
\begin{itemize}
  \item An LLM is being fine tuned through reinforcement learning through human feedback. The states are a numerical representation of the inputted responses from a human and a numerical representation of the LLM's responses and it's weights. The action of the MDP is a shift in each of the weights and a novel generated prompt attempting to correct the response from earlier. The agent is given a reward that corresponds to a sentiment analysis of the human's response to the correction.
  \item A robot must learn to replicate the movements of a human. The states would be a representation of the human's movements. One way of this could be fixing dots on joints of the human and feeding the position of the dots to the robot. The robot then uses these dots and its own dots to move it's body in anticipation of the human's movements. The actions of the robot are movements of its possible joints, these may not correspond 100\% to the human dots or it's own dots if there are differences in how it can move it and how its movement is represented. The robot could be given a reward that is inversely proportional to the distance of its dots and the humans.  
  \item An AI must solve through a complex video game like final fantasy 7. The states would be incredibly complex as it would have to be designed with great detail and consideration. The state could include features such as: The screen (or a CNN output of the screen to reduce number of states), the items and materia in possession, the state of battle (0 if not in battle or 1 if in battle), the number of characters in the party. The actions could be the buttons that the player can press on a controller like start, o, x, and stick direction. The reward structure has a lot of freedom. The AI could be intrinsically motivated with curiosity through rewarding it based on an unseen state, or it could be given a bonus proportional to the EXP and gold earned.   
\end{itemize}

\subsection{Exercise 3.2}
It is not, the markov property assumes that the current state is a unique sufficient representation for the agent's past and its future. This could be violated in many scenarios. One of them could be a robot that is designed to escape a maze and it is given the state representation of the $[s_1,s_2,s_3,s_4]$ where $s_i = 1$ if the robot senses a wall within 3 feet of it's current position in each direction north, east, south, west. There are likely many parts of the maze that have identical state representations and yet are completely different parts of the maze. However, many MDPs can be turned into MDP if given a state that reflects it's history. For example, in this robot you could include a vector containing each of it's prior actions as a state and would therefore be able to know if it is repeating movements.

\subsection{Exercise 3.3}
The answer really depends on the underlying goal of the person creating the agent-environment system. In this driving scenario, it is likely that the person already knows where they are driving to, the just don't know how to get there. So an incredibly high level would not be a great place to draw the line because it wouldn't align the agent's actions with the designer's goals. 

\subsection{Exercise 3.4}

\begin{tabular}{ c c c c| c}
  \hline
$s$ & $a$ & $s^\prime$ & $r$ & $p(s^\prime,r|s,a)$ \\
\hline
high & search & high & $r_{search}$ & $\alpha$ \\
high & search & low & $r_{search}$ & $1-\alpha$ \\
high & wait & high & $r_{wait}$ & $1$ \\
low & search & low & $r_{search}$ & $\beta$ \\
low & search & high & $-3$ & $1-\beta$ \\
low & recharge & high & $0$ & $1$ \\
low & wait & low & $r_{wait}$ & $1$ \\
\hline
\end{tabular}

\subsection{Exercise 3.5}
Equation 3.3 is:
\begin{equation}
  \sum_ {s^\prime \in S} \sum_{r \in R} p(s^\prime, r | s,a) = 1, \forall s \in S, a \in A(s)
\end{equation}

Modifying this for the episodic case:
\begin{equation}
  \sum_ {s^\prime \in S^+} \sum_{r \in R} p(s^\prime, r | s,a) = 1, \forall s \in S, a \in A(s)
\end{equation}

\subsection{Exercise 3.6}
The return for this discounted episodic case would be $-\gamma^{K-1}$ where $K$ is the number of time steps in the episode, normally $T$ in episodic cases but for the sake of comparison we use $K$. This is identical to the discounted continuous case.

\subsection{Exercise 3.7}
Since this is an episodic case and is not discounted, the robot will receive a reward of 1 every time it escapes the maze no matter the amount of time spent in the maze. The fact that the return at each time step is 1 no matter the state the robot is in or the action it chooses. The robot will eventually escape the maze and so there is no hurry in finding an optimal path.A better approach would be to give the robot a negative reward for every time step in order to incentivize the robot to escape as soon as possible.

\subsection{Exercise 3.8}
$\gamma = 0.5$ and $ R_1=-1, R_2=2, R_3=6, R_4=3, R_5=2$ and $T=5$

\begin{gather}
  G_0 = -1 + 0.5*2 + 0.5^2 * 6 + 0.5^3 * 3 + 0.5^4*2 = 2 \\
  G_1 = 2 + 0.5 * 6 + 0.5^2 * 3 + 0.5^3*2 = 6 \\
  G_2 = 6 + 0.5 * 3 + 0.5^2*2 = 8 \\
  G_3 = 3 + 0.5*2 = 4 \\
  G_4 = 2 = 2 \\
  G_5 = 0 = 0
\end{gather}

\subsection{Exercise 3.9}
$\gamma = 0.9, R_1 = 2, R_i = 7 \text{ }\forall i \in [2,\infty]$

\begin{gather}
  G_0 = 2 + \sum_{i=2}^{\infty} 0.9^{i-1}7 = 2 - 7 + 7\sum_{i=0}^{\infty} 0.9^{i} \\
  = -5 + 7/0.1 = 65 \\
  G_1 = \sum_{i=1}^{\infty} 0.9^{i-1}7 = 7\sum_{i=0}^{\infty} 0.9^{i} \\
  = 7/0.1 = 70
\end{gather}

\subsection{Exercise 3.10}
Proving $\sum_{k=0}^{\infty}\gamma^k = \frac{1}{1-\gamma}$: 
\begin{gather}
  \sum_{k=0}^{\infty}\gamma^k = \lim_{k \to \infty}\gamma^0 + \gamma^1 + \gamma^2 + \dots + \gamma^k \\
  \lim_{k \to \infty}\gamma^0 + \gamma^1 + \gamma^2 + \dots + \gamma^k = \lim_{k \to \infty}\gamma^0 + \gamma^1 + \gamma^2 + \dots + \gamma^k + \gamma^{k+1} \\
  (1-\gamma)\lim_{k \to \infty}\gamma^0 + \gamma^1 + \gamma^2 + \dots + \gamma^k = (1-\gamma)\lim_{k \to \infty}\gamma^0 + \gamma^1 + \gamma^2 + \dots + \gamma^k + \gamma^{k+1} \\
  = 1+ \lim_{k \to \infty}(\gamma-\gamma)^0 + (\gamma-\gamma)^1 + (\gamma-\gamma)^2 + \dots + (\gamma-\gamma)^k + \gamma^{k+1} \\
  = 1 + 0 \\
  (1-\gamma)\sum_{k=0}^{\infty}\gamma^k = 1 \\
  \sum_{k=0}^{\infty}\gamma^k = \frac{1}{1-\gamma}
\end{gather}

\subsection{Exercise 3.11}
Finding $E[r|\pi,p]$ where $\pi$ is a stochastic mapping of states to actions and $p(s^\prime,r|s,a) = Pr(S_t = s^\prime, R_t = r  | S_{t-1} = s, A_{t-1} = a )$

\begin{gather}
  E[r|\pi, p, S_t = s] = \sum_a \pi(a|S_t) \sum_{s^\prime, r} r p(s^\prime, r | s,a) 
\end{gather}

\subsection{Exercise 3.12}
Giving an equation for $v_\pi$ in terms of $q_\pi$ and $\pi$:

\begin{equation}
  v_\pi(s) = \sum_a q_\pi(s,a) \pi(a|s)
\end{equation}

\subsection{Exercise 3.13}
Giving an equation for $q_\pi$ in terms of $v_\pi$ and $p$: 

\begin{equation}
  q_\pi(s,a) = \sum_{s^\prime} \sum_r p(s^\prime,r|s,a)[r+\gamma v_\pi(s^\prime)]
\end{equation}

\subsection{Exercise 3.14}
\begin{gather}
  v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s^\prime,r} p(s^\prime, r | s,a)[r + \gamma v_\pi(s^\prime)] \\
  \text{Verifying this for the gridworld example} \\
  0.7 = 0.25*1[0 + 0.9*0.7] + 0.25[0.9*2.3] + 0.25[0.9*0.4] + 0.25[0.9*(-0.4)] \\
  0.7 = \frac{63}{400} + \frac{207}{400} \\
  0.7 =  \frac{270}{400} \approx 0.7
\end{gather}

\subsection{Exercise 3.15}
Adding a constant $c$ to the rewards:
\begin{gather}
  v_\pi(s) = E[G_t | S_t=s]\\ 
  v_\pi = E[\sum_{k=0}^\infty \gamma^k (R_{t+k+1} + c) | S_t = s] \\
  = E[\sum_{k=0}^\infty \gamma^k R_{t+k+1} + \sum_{k=0}^\infty \gamma^k c | S_t = s] \\
  = E[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t = s] + \frac{c}{1-\gamma} \\
  v_\pi(s) = v_\pi(s)+ \frac{c}{1-\gamma}
\end{gather}

\subsection{Exercise 3.16}
Adding a reward can change the task in an episodic case as imagine if in the gridworld case, the user was given a -0.1 reward for going north, if the periodic case worked out where on the second to last time step, the user landed in $A^\prime$, normally it's best action would be to travel north in order to get access to the reward associated with $A \rightarrow A^\prime$. However, adding a total constant would change the appeal of going north.

\subsection{Exercise 3.17}
Finding $q_\pi(s,a)$bellman equation

\begin{gather}
  q_\pi(s,a) = E_\pi[G_t | S_t=s, A_t = a] \\
  q_\pi(s,a) = E_\pi[R_{t+1} + \gamma G_{t+1} | S_t=s, A_t = a] \\
  q_\pi(s,a) = E_\pi[R_{t+1} | S_t= s, A_t = a] + E_\pi[\gamma G_{t+1} | S_t=s, A_t = a] \\
  = \sum_{s^\prime,r} p(s^\prime, r | s,a)[r + \gamma \sum_{a^\prime} \pi(a^\prime|s^\prime) q_\pi(s^\prime,a^\prime)]
\end{gather}

\subsection{Exercise 3.18}
Equation of expected state value based off state action pairs. 
\begin{equation}
  v_\pi(s) = E_\pi[q_\pi(s,a)]
\end{equation}

Equation without the expectation
\begin{equation}
  v_\pi(s) = \sum_{a}\pi(a|s) q_\pi(s,a)
\end{equation}

\subsection{Exercise 3.19}
Equation of expected state-action value based off the future states and rewards.
\begin{equation}
  q_\pi(s,a) = E[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a ]
\end{equation}

Equation without the expectation
\begin{equation}
  q_\pi(s,a) = \sum_{s^\prime} \sum_r p(s^\prime, r| s,a)[r+\gamma v_\pi(s^\prime)]
\end{equation}

\subsection{Exercise 3.20}
Describing the optimal state-value function for the golf example.

\begin{equation}
  v_*(s) = \max_a \sum_{s^\prime, r} p(s^\prime, r | s,a)[r + \gamma v_*(s^\prime)]
\end{equation}

The optimal state value function would require summing over each state from a given state and weighing it by the probability of it occuring based off each action. So an example is for the -6 state, the possible actions are putt or driver. The state-value function would include the impossibility of moving from -6 to -4 using a putt and moving from -6 to -5. The way this would look is similar to the lower part of figure 3.3 with the exception that optimally, if the ball landed in the green the optimal action is to switch to putt. The diagram would then look exactly like the $q_*(s,driver)$ drawing but the -1 circle would be expanded to be the same as the $v_{putt}$ drawing.

\subsection{Exercise 3.21}
Describing the optimal action-value function for putting in the golf example. 

\begin{equation}
  q_*(s,a) = \sum_{s^\prime, r} p(s^\prime, r | s,a)[r+\gamma \max_{a^\prime} q_*(s^\prime, a^\prime)]
\end{equation}

The drawing would likely look similar to the $q_*(s,driver)$ with the exception that the -6 sliver at the tee would be -4 and the contour lines would shift inward from the s driver drawing.

\subsection{Exercise 3.22}
If $\gamma = 0$, the optimal policy would be $\pi_{left}$ since the 0 discount rate would cause the value function for the state to be 1 for left and 0 for the right. 

If $\gamma = 0.9$, the optimal policy would be $\pi_{right}$ since the value for the state would be:

\begin{gather}
  v_{right}(s) = \sum_{k=0}^\infty 0.9^{2k} * 0 + \sum_{k=0}^\infty 0.9^{2k+1}*2 \\
  = 1.8\sum_{k=0}^\infty 0.9^{2k} = 1.8 * \frac{1}{1-0.81}\\
  = \frac{1.8}{0.19} \approx 9.48
\end{gather}

\begin{gather}
  v_{left}(s) = \sum_{k=0}^\infty 0.9^{2k} * 1 + \sum_{k=0}^\infty 0.9^{2k+1}*0 \\
  = \frac{1}{1-0.81} \approx 5.26
\end{gather}

If $\gamma = 0.5$, both policies are the optimal policy.

\begin{gather}
  v_{right}(s) = \sum_{k=0}^\infty 0.5^{2k} * 0 + \sum_{k=0}^\infty 0.5^{2k+1}*2 \\
  = \frac{1}{1-0.25} = 4
\end{gather}

\begin{gather}
  v_{right}(s) = \sum_{k=0}^\infty 0.5^{2k} * 1 + \sum_{k=0}^\infty 0.5^{2k+1}*0 \\
  = \frac{1}{1-0.25} = 4
\end{gather}

\subsection{Exercise 3.23}
Bellman equation for q:
\begin{equation}
  q_*(s,a) = \sum_{s^\prime, r} p(s^\prime, r | s,a)[r+\gamma \max_{a^\prime} q_*(s^\prime, a^\prime)]
\end{equation}

Finding one bellman equation for the recycling robot, there are 6:

\begin{gather}
  q_*(high,search) = \alpha[r_{search}+\gamma \max_{a^\prime}(q_*(high,search),q_*(high,wait))] \\+ (1-\alpha)[r_{search}+\gamma \max_{a^\prime}(q_*(low,search),q_*(low,wait),q_*(low,recharge))]
\end{gather}

\subsection{Exercise 3.24}
Optimal policy is to travel to $A$, which would bring the agent to $A^\prime$ and then go back north to A and repeat infinitely.
\begin{gather}
  v_\pi(s_A) =  E_\pi[\sum_{k=0}^\infty \gamma^k R_{k+t+1}|S_t = s] \\
  = 10 \sum_{k=0}^\infty 0.9^{5k} \\
  = \frac{10}{1-0.9^5} \\
  = 24.419
\end{gather}

\subsection{Exercise 3.25}

\begin{gather}
  v_*(s) = \sum_{s^\prime,r} p(s^\prime, r | s,a)[r + \gamma \max_{a^\prime} q(s^\prime, a^\prime)] \\
  v_*(s) = \max_{a^\prime} q(s,a^\prime)
\end{gather}

\subsection{Exercise 3.26}

\begin{gather}
  q_*(s,a) = \sum_{s^\prime, r} p(s^\prime,r|s,a)[r+\gamma v_*(s^\prime)]
\end{gather}

\subsection{Exercise 3.27}

\begin{gather}
  \pi_*(a|s) = \arg \max_a q(s,a) 
\end{gather}

\subsection{Exercise 3.28}

\begin{gather}
  \pi_*(a|s) = \arg \max_a \sum_{s^\prime, r} p(s^\prime,r|s,a)[r+\gamma v_*(s^\prime)]
\end{gather}

\subsection{Exercise 3.29}

\begin{equation}
  v_\pi(s) = \sum_{a}\pi(a|s) \sum_{s^\prime} p(s^\prime | s,a)[r(s,a) + \gamma v_\pi(s^\prime)] 
\end{equation}

\begin{equation}
  v_*(s) = \max_a \sum_{s^\prime} p(s^\prime | s,a)[r(s,a) + \gamma v_\pi(s^\prime)] 
\end{equation}

\begin{equation}
  q_\pi(s,a) = r(s,a) + \gamma \sum_{s^\prime} p(s^\prime|s,a) \sum_{a^\prime} q_\pi(s^\prime,a^\prime) \pi(a^\prime|s^\prime)
\end{equation}

\begin{equation}
  q_\pi(s,a) = r(s,a) + \gamma \sum_{s^\prime} p(s^\prime|s,a) \sum_{a^\prime} q_*(s^\prime,a^\prime) \pi(a^\prime|s^\prime)
\end{equation}