\section{Multi-armed Bandits}

\subsection{Exercise 2.1}
Define $a_1$, $a_2$ as the two actions that can be taken, $\varepsilon$ as the probability of selecting a random action, and $r$ as a randomly generated number $\in [0,1]$. 
\begin{equation}
    \text{action} = 
    \begin{cases}
        \text{random choice of $a_1$ or $a_2$} & \text{if } r < \varepsilon \\
        \text{greedy choice} & \text{if } r \geq \varepsilon
        \end{cases}
\end{equation}

if $\varepsilon$ is 0.5, there is a 0.5 probability of taking the greedy choice and 0.5 probability of picking the random choice. The random choice has two actions, therefore there is a choice of picking $a_1$ or $a_2$ with 0.25 prob each. $0.25 + 0.50 = 0.75$, therefore the probability of the greedy action getting selected is 0.75

\subsection{Exercise 2.2}
$k$-armed bandits with $k$ = 4. $\varepsilon$-greedy algorithm setting $Q_1(a) = 0 \text{ }\forall a$.
Sample a trajectory $(a_1,r_1,a_2,\dots) \text{ } as \text{ } (1,-1, 2,1, 2,-2,2,2,3,0)$

\begin{itemize}
  \item $A_1 = 1$, Greedy (tie broken) or random
  \item $A_2 = 2$, Greedy (tie broken) or random
  \item $A_3 = 2$, Greedy or random
  \item $A_4 = 2$, Must have been random because average reward of $a_2$ is now $-1/2$
  \item $A_5 = 3$, Greedy (tie broken) or random
\end{itemize}

\subsection{Exercise 2.3}
In the long run, the $\varepsilon = 0.01$ will perform the best. On average, it will pick the optimal action more frequently while still sampling each action an infinite number of times. In the long run, as $Q(a) \approx q_*(a), $ $\varepsilon= 0.01$ will sample the optimal action 99.1\% of the time,  whereas $\varepsilon = 0.1$ will sample the optimal action 91\% of the time, resulting in a smaller frequency of the optimal action being picked and a smaller total reward over the long run. 